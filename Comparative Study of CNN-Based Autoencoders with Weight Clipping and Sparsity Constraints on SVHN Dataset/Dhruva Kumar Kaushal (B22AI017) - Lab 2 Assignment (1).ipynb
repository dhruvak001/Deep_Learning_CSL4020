{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.11.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (1.11.4)\n",
      "Requirement already satisfied: networkx>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=10.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (11.0.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (2025.1.10)\n",
      "Requirement already satisfied: packaging>=21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image) (0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Using downloaded and verified file: ./data/train_32x32.mat\n",
      "Using downloaded and verified file: ./data/test_32x32.mat\n",
      "Training with ReLU and Adam\n",
      "Epoch [1/10], Train Loss: 0.0059, Test Loss: 0.0019, PSNR: 28.9237\n",
      "Epoch [2/10], Train Loss: 0.0015, Test Loss: 0.0011, PSNR: 31.5255\n",
      "Epoch [3/10], Train Loss: 0.0011, Test Loss: 0.0009, PSNR: 31.9981\n",
      "Epoch [4/10], Train Loss: 0.0009, Test Loss: 0.0008, PSNR: 33.0110\n",
      "Epoch [5/10], Train Loss: 0.0008, Test Loss: 0.0007, PSNR: 33.4674\n",
      "Epoch [6/10], Train Loss: 0.0006, Test Loss: 0.0006, PSNR: 33.4732\n",
      "Epoch [7/10], Train Loss: 0.0005, Test Loss: 0.0004, PSNR: 35.5056\n",
      "Epoch [8/10], Train Loss: 0.0005, Test Loss: 0.0004, PSNR: 35.8557\n",
      "Epoch [9/10], Train Loss: 0.0004, Test Loss: 0.0007, PSNR: 32.0606\n",
      "Epoch [10/10], Train Loss: 0.0004, Test Loss: 0.0005, PSNR: 33.5775\n",
      "Best PSNR: 35.8557\n",
      "Epoch [1/10], Train Loss: 0.4459, Test Loss: 0.0523, PSNR: 14.1272\n",
      "Epoch [2/10], Train Loss: 0.1004, Test Loss: 0.0523, PSNR: 14.1232\n",
      "Epoch [3/10], Train Loss: 0.1004, Test Loss: 0.0523, PSNR: 14.1252\n",
      "Epoch [4/10], Train Loss: 0.1004, Test Loss: 0.0524, PSNR: 14.1198\n",
      "Epoch [5/10], Train Loss: 0.1004, Test Loss: 0.0524, PSNR: 14.1225\n",
      "Epoch [6/10], Train Loss: 0.1004, Test Loss: 0.0522, PSNR: 14.1323\n",
      "Epoch [7/10], Train Loss: 0.1004, Test Loss: 0.0522, PSNR: 14.1356\n",
      "Epoch [8/10], Train Loss: 0.1004, Test Loss: 0.0523, PSNR: 14.1292\n",
      "Epoch [9/10], Train Loss: 0.1004, Test Loss: 0.0523, PSNR: 14.1265\n",
      "Epoch [10/10], Train Loss: 0.1004, Test Loss: 0.0523, PSNR: 14.1277\n",
      "Best PSNR: 14.1356\n",
      "Training with ReLU and RMSprop\n",
      "Epoch [1/10], Train Loss: 0.0064, Test Loss: 0.0031, PSNR: 26.4018\n",
      "Epoch [2/10], Train Loss: 0.0024, Test Loss: 0.0017, PSNR: 28.8429\n",
      "Epoch [3/10], Train Loss: 0.0017, Test Loss: 0.0018, PSNR: 28.5096\n",
      "Epoch [4/10], Train Loss: 0.0014, Test Loss: 0.0013, PSNR: 29.9219\n",
      "Epoch [5/10], Train Loss: 0.0013, Test Loss: 0.0013, PSNR: 29.8202\n",
      "Epoch [6/10], Train Loss: 0.0012, Test Loss: 0.0009, PSNR: 31.7872\n",
      "Epoch [7/10], Train Loss: 0.0011, Test Loss: 0.0008, PSNR: 32.0768\n",
      "Epoch [8/10], Train Loss: 0.0010, Test Loss: 0.0010, PSNR: 31.1684\n",
      "Epoch [9/10], Train Loss: 0.0009, Test Loss: 0.0007, PSNR: 32.7245\n",
      "Epoch [10/10], Train Loss: 0.0009, Test Loss: 0.0007, PSNR: 32.4496\n",
      "Best PSNR: 32.7245\n",
      "Epoch [1/10], Train Loss: 0.3530, Test Loss: 0.0522, PSNR: 14.1342\n",
      "Epoch [2/10], Train Loss: 0.2655, Test Loss: 0.0523, PSNR: 14.1275\n",
      "Epoch [3/10], Train Loss: 0.2639, Test Loss: 0.0523, PSNR: 14.1223\n",
      "Epoch [4/10], Train Loss: 0.2643, Test Loss: 0.0522, PSNR: 14.1360\n",
      "Epoch [5/10], Train Loss: 0.2644, Test Loss: 0.0523, PSNR: 14.1283\n",
      "Epoch [6/10], Train Loss: 0.2639, Test Loss: 0.0523, PSNR: 14.1241\n",
      "Epoch [7/10], Train Loss: 0.2639, Test Loss: 0.0524, PSNR: 14.1211\n",
      "Epoch [8/10], Train Loss: 0.2645, Test Loss: 0.0523, PSNR: 14.1280\n",
      "Epoch [9/10], Train Loss: 0.2645, Test Loss: 0.0522, PSNR: 14.1318\n",
      "Epoch [10/10], Train Loss: 0.2635, Test Loss: 0.0523, PSNR: 14.1278\n",
      "Best PSNR: 14.1360\n",
      "Training with LeakyReLU and Adam\n",
      "Epoch [1/10], Train Loss: 0.0062, Test Loss: 0.0025, PSNR: 27.2678\n",
      "Epoch [2/10], Train Loss: 0.0015, Test Loss: 0.0011, PSNR: 31.4038\n",
      "Epoch [3/10], Train Loss: 0.0010, Test Loss: 0.0009, PSNR: 32.3003\n",
      "Epoch [4/10], Train Loss: 0.0009, Test Loss: 0.0009, PSNR: 31.9668\n",
      "Epoch [5/10], Train Loss: 0.0007, Test Loss: 0.0006, PSNR: 34.0655\n",
      "Epoch [6/10], Train Loss: 0.0007, Test Loss: 0.0006, PSNR: 34.4827\n",
      "Epoch [7/10], Train Loss: 0.0005, Test Loss: 0.0004, PSNR: 36.0998\n",
      "Epoch [8/10], Train Loss: 0.0004, Test Loss: 0.0004, PSNR: 36.2104\n",
      "Epoch [9/10], Train Loss: 0.0004, Test Loss: 0.0003, PSNR: 37.2747\n",
      "Epoch [10/10], Train Loss: 0.0004, Test Loss: 0.0003, PSNR: 37.3279\n",
      "Best PSNR: 37.3279\n",
      "Epoch [1/10], Train Loss: 0.4480, Test Loss: 0.0523, PSNR: 14.1308\n",
      "Epoch [2/10], Train Loss: 0.1006, Test Loss: 0.0523, PSNR: 14.1288\n",
      "Epoch [3/10], Train Loss: 0.1006, Test Loss: 0.0523, PSNR: 14.1284\n",
      "Epoch [4/10], Train Loss: 0.1006, Test Loss: 0.0524, PSNR: 14.1213\n",
      "Epoch [5/10], Train Loss: 0.1006, Test Loss: 0.0523, PSNR: 14.1270\n",
      "Epoch [6/10], Train Loss: 0.1006, Test Loss: 0.0523, PSNR: 14.1279\n",
      "Epoch [7/10], Train Loss: 0.1006, Test Loss: 0.0523, PSNR: 14.1274\n",
      "Epoch [8/10], Train Loss: 0.1006, Test Loss: 0.0523, PSNR: 14.1306\n",
      "Epoch [9/10], Train Loss: 0.1006, Test Loss: 0.0524, PSNR: 14.1203\n",
      "Epoch [10/10], Train Loss: 0.1006, Test Loss: 0.0523, PSNR: 14.1313\n",
      "Best PSNR: 14.1313\n",
      "Training with LeakyReLU and RMSprop\n",
      "Epoch [1/10], Train Loss: 0.0062, Test Loss: 0.0027, PSNR: 26.9323\n",
      "Epoch [2/10], Train Loss: 0.0021, Test Loss: 0.0028, PSNR: 27.0021\n",
      "Epoch [3/10], Train Loss: 0.0015, Test Loss: 0.0012, PSNR: 30.1635\n",
      "Epoch [4/10], Train Loss: 0.0013, Test Loss: 0.0016, PSNR: 28.6492\n",
      "Epoch [5/10], Train Loss: 0.0011, Test Loss: 0.0010, PSNR: 30.9192\n",
      "Epoch [6/10], Train Loss: 0.0010, Test Loss: 0.0007, PSNR: 32.7151\n",
      "Epoch [7/10], Train Loss: 0.0009, Test Loss: 0.0005, PSNR: 33.7935\n",
      "Epoch [8/10], Train Loss: 0.0008, Test Loss: 0.0008, PSNR: 31.9742\n",
      "Epoch [9/10], Train Loss: 0.0007, Test Loss: 0.0007, PSNR: 32.3069\n",
      "Epoch [10/10], Train Loss: 0.0007, Test Loss: 0.0010, PSNR: 30.9028\n",
      "Best PSNR: 33.7935\n",
      "Epoch [1/10], Train Loss: 0.3593, Test Loss: 0.0524, PSNR: 14.1213\n",
      "Epoch [2/10], Train Loss: 0.2750, Test Loss: 0.0523, PSNR: 14.1285\n",
      "Epoch [3/10], Train Loss: 0.2750, Test Loss: 0.0524, PSNR: 14.1220\n",
      "Epoch [4/10], Train Loss: 0.2750, Test Loss: 0.0522, PSNR: 14.1375\n",
      "Epoch [5/10], Train Loss: 0.2750, Test Loss: 0.0523, PSNR: 14.1309\n",
      "Epoch [6/10], Train Loss: 0.2750, Test Loss: 0.0523, PSNR: 14.1312\n",
      "Epoch [7/10], Train Loss: 0.2750, Test Loss: 0.0523, PSNR: 14.1248\n",
      "Epoch [8/10], Train Loss: 0.2750, Test Loss: 0.0523, PSNR: 14.1292\n",
      "Epoch [9/10], Train Loss: 0.2750, Test Loss: 0.0522, PSNR: 14.1324\n",
      "Epoch [10/10], Train Loss: 0.2749, Test Loss: 0.0523, PSNR: 14.1258\n",
      "Best PSNR: 14.1375\n",
      "Configuration: ReLU_Adam\n",
      "Weight Clipping Only - Best PSNR: 35.8557\n",
      "Weight Clipping + L1 Sparsity - Best PSNR: 14.1356\n",
      "\n",
      "Configuration: ReLU_RMSprop\n",
      "Weight Clipping Only - Best PSNR: 32.7245\n",
      "Weight Clipping + L1 Sparsity - Best PSNR: 14.1360\n",
      "\n",
      "Configuration: LeakyReLU_Adam\n",
      "Weight Clipping Only - Best PSNR: 37.3279\n",
      "Weight Clipping + L1 Sparsity - Best PSNR: 14.1313\n",
      "\n",
      "Configuration: LeakyReLU_RMSprop\n",
      "Weight Clipping Only - Best PSNR: 33.7935\n",
      "Weight Clipping + L1 Sparsity - Best PSNR: 14.1375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Define the Autoencoder Architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # Output: 16x16x16\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # Output: 8x8x32\n",
    "            activation_fn(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Output: 4x4x64\n",
    "            activation_fn()\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 8x8x32\n",
    "            activation_fn(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 16x16x16\n",
    "            activation_fn(),\n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # Output: 32x32x3\n",
    "            nn.Sigmoid()  # Ensure output is in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define Weight Clipping Function\n",
    "def clip_weights(model, min_val=-0.5, max_val=0.5):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.clamp_(min_val, max_val)\n",
    "\n",
    "# Load and Preprocess SVHN Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training Function with Early Stopping\n",
    "def train_autoencoder(model, train_loader, test_loader, optimizer, criterion, epochs=100, use_sparsity=False, sparsity_lambda=0.01, patience=10):\n",
    "    train_losses, test_losses = [], []\n",
    "    best_psnr = 0.0\n",
    "    best_test_loss = float('inf')\n",
    "    epochs_no_improve = 0  # Counter for epochs without improvement\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            images, _ = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            \n",
    "            if use_sparsity:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += sparsity_lambda * l1_norm\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            clip_weights(model)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        psnr_score = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                images, _ = batch\n",
    "                outputs = model(images)\n",
    "                test_loss += criterion(outputs, images).item()\n",
    "                for i in range(images.size(0)):\n",
    "                    psnr_score += psnr(images[i].cpu().numpy(), outputs[i].cpu().numpy(), data_range=1.0)\n",
    "        \n",
    "        epoch_loss /= len(train_loader)\n",
    "        test_loss /= len(test_loader)\n",
    "        psnr_score /= len(test_dataset)\n",
    "        \n",
    "        # Check for improvement in test loss\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            epochs_no_improve = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_no_improve += 1  # Increment counter\n",
    "        \n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} as test loss did not improve for {patience} epochs.\")\n",
    "            break\n",
    "        \n",
    "        if psnr_score > best_psnr:\n",
    "            best_psnr = psnr_score\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}, PSNR: {psnr_score:.4f}\")\n",
    "    \n",
    "    print(f\"Best PSNR: {best_psnr:.4f}\")\n",
    "    return train_losses, test_losses, best_psnr\n",
    "\n",
    "# Train and Compare Models\n",
    "activation_fns = [nn.ReLU, nn.LeakyReLU]\n",
    "optimizers = [optim.Adam, optim.RMSprop]\n",
    "epochs = 10\n",
    "sparsity_lambda = 0.01\n",
    "patience = 10  # Early stopping patience\n",
    "\n",
    "results = {}\n",
    "for activation_fn in activation_fns:\n",
    "    for optimizer_fn in optimizers:\n",
    "        print(f\"Training with {activation_fn.__name__} and {optimizer_fn.__name__}\")\n",
    "        \n",
    "        # Model 1: Weight clipping only\n",
    "        model1 = ConvAutoencoder(activation_fn)\n",
    "        optimizer1 = optimizer_fn(model1.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "        train_losses1, test_losses1, psnr1 = train_autoencoder(model1, train_loader, test_loader, optimizer1, criterion, epochs, patience=patience)\n",
    "        \n",
    "        # Model 2: Weight clipping + L1 sparsity\n",
    "        model2 = ConvAutoencoder(activation_fn)\n",
    "        optimizer2 = optimizer_fn(model2.parameters(), lr=0.001)\n",
    "        train_losses2, test_losses2, psnr2 = train_autoencoder(model2, train_loader, test_loader, optimizer2, criterion, epochs, use_sparsity=True, sparsity_lambda=sparsity_lambda, patience=patience)\n",
    "        \n",
    "        key = f\"{activation_fn.__name__}_{optimizer_fn.__name__}\"\n",
    "        results[key] = {\n",
    "            \"Weight Clipping Only\": {\"Train Loss\": train_losses1, \"Test Loss\": test_losses1, \"PSNR\": psnr1},\n",
    "            \"Weight Clipping + L1 Sparsity\": {\"Train Loss\": train_losses2, \"Test Loss\": test_losses2, \"PSNR\": psnr2}\n",
    "        }\n",
    "\n",
    "# Compare results\n",
    "for key, value in results.items():\n",
    "    print(f\"Configuration: {key}\")\n",
    "    print(f\"Weight Clipping Only - Best PSNR: {value['Weight Clipping Only']['PSNR']:.4f}\")\n",
    "    print(f\"Weight Clipping + L1 Sparsity - Best PSNR: {value['Weight Clipping + L1 Sparsity']['PSNR']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
